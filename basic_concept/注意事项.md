# multiprocessing
## 多进程爆内存
在使用多线程时，比如下面示例，只返回一个result，需要用zip(*result) 解析出来函数返回出来的所有参数。map 会将等待所有的任务完成，才返回，如果返回数据（尤其是gpu）会导致内存爆炸，建议用imap或者类似的。解析出来的结果是列表，包含每个进程的结果。如果使用用imap或者类似的 ， 也是列表，只不过长度为1


```

from multiprocessing import Pool
def square(x):
    return x ** 2
with Pool(4) as p:
    results = p.map(square, [1, 2, 3, 4])
```
## 多进程，每个进程使用不同的GPU，需要修改一下协议
* 方法1：设置协议
```
  mp.set_start_method('spawn')
```
* 方法2：在进程内部，设置仅某个GPU 可见,在 进程函数里面，先设置可见性
```
def worker(rank):
    # 设置环境变量，限制当前进程只能看到指定 GPU
    os.environ["CUDA_VISIBLE_DEVICES"] = str(rank)
    # 此时 device_id=0 对应实际的 GPU rank
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

```

#  多GPU 训练
##  PyTorch 数据并行（DataParallel）
优点：简单易用，只需一行代码。

缺点：单进程多线程，效率较低；负载不均衡（主 GPU 负责聚合梯度）。
使用方法： 用下面代码 即可，
```
 model=nn.DataParallel(model)
```
## DDP PyTorch 分布式数据并行（DistributedDataParallel, DDP）
优点：多进程并行，效率高；负载均衡。
缺点：配置稍复杂，需初始化进程组。
使用方法：
* 用ddp 封装模型
```
    model = models.resnet50().to(rank)
    ddp_model = DDP(model, device_ids=[rank])
```
*   数据采样需要封装,验证集 可以封装，也可以不封装。不封装，只在主线程验证；如果封装，则需要 收集每个模型的验证结果，求平均
```
    train_dataset = datasets.ImageFolder('path/to/data')
    train_sampler = torch.utils.data.distributed.DistributedSampler(
        train_dataset,
        num_replicas=world_size,
        rank=rank
    )
    train_loader = DataLoader(
        train_dataset,
        batch_size=32,
        sampler=train_sampler
    )
```
