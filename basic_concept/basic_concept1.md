阅读使人充实，会谈使人敏捷，写作与笔记使人精确。 —— 培根
康奈尔笔记法（Cornell Notes System）是由美国康奈尔大学教授沃尔特·波克发明的一种笔记方法。
将笔记纸分为 3 个栏目：笔记、关键词、总结，来高效记笔记，并便于复习巩固知识点。实际操作：
1. 用简洁的语句在笔记栏记下重点知识。
2. 将笔记栏中的知识点提炼为关键词和要点，写在关键词栏。在总结栏写下本次学习总结。
3. 遮盖笔记栏的内容，通过关键词栏的要点概括，复述出笔记内容来巩固知识。
1、概念
关键词
- CAM 其实就是将gap（全局平均池化的结果作为权重，线性加权gap层之前的所有channel，得到一热力图，用于表示对哪里的关注比较大）
- …












常见的模型可视化分析方法
1. class Activation Mapping（CAM，类激活映射）是一种用于卷积神经网络（CNN）模型的可视化技术，主要用于解释模型在图像分类任务中关注的区域，从而实现对目标的定位和分类的联合理解。CAM最早由论文《Learning Deep Features for Discriminative Localization》（2016 CVPR）提出。
2. Grad-CAM：
输入：给定的一个图像和一个感兴趣的类别（ 例如：tiger cat）；
通过模型的 CNN 部分进行向前传播，得到特定任务的各类别分数 y（ softmax 层之前 ）；
将给定的类别（tiger cat）设置为 1，其他类别的梯度都设置为 0 ；
将给定类别分数 yc 反向传播至卷积特征图，组合计算得到粗糙的梯度CAM定位（蓝色热力图）；
将热力图与反向传播的结果进行点乘，得到高分辨率的特定 Grad-CAM 可视化图 ；（与IG的归因分析比较相似，相当于只对一张图求解梯度）


3. Path-based attribution methods, such as integrated
gradients (IG) [8] that is based on game-theoretic idea [13], offer a valuable tool for XAI by accumulating
gradients along a path from a baseline image to the target image being explained.

4. Guided Backpropagation
Guided Backpropagation（导向反向传播）是一种用于神经网络可视化的技术，主要目的是揭示输入图像中哪些部分对网络的最终决策起到了关键作用。它是在传统反向传播的基础上进行改进，通过修改ReLU激活函数的反向传播过程，使得只有正向传播中激活值和反向传播中梯度均为正的部分才能通过，从而过滤掉负梯度，得到更细粒度、更直观的特征可视化结果。
具体来说，Guided Backpropagation的核心思想是：
修改ReLU的反向传播规则：在反向传播时，传统的ReLU只会将正的梯度传递下去，而Guided Backpropagation进一步要求正向传播的激活值也必须大于0，只有满足这两个条件的梯度才被保留并传递。这种双重筛选机制使得梯度更加稀疏且聚焦于对分类结果有积极贡献的特征。
可视化梯度：通过这种方式得到的梯度图可以反映输入图像中哪些像素对特定类别的激活贡献最大，从而帮助理解模型的判别依据。
5. 亮度过渡问题（Brightness Transition Problem）
定义：

亮度过渡问题是指在图像融合过程中，由于缺乏对图像整体（全局）亮度的把控，导致融合结果在不同区域之间出现不自然的亮度跳变或突变，表现为图像某些区域明暗变化突兀，缺乏平滑过渡。
在MEF中的表现：

在多曝光融合中，像素级方法通常只关注单个像素的亮度信息进行加权平均或选择。由于没有考虑周围像素和全局场景的关系，容易导致相邻区域间亮度不连续。例如，暗区和亮区的接壤处会出现明显的分界线，使得融合图像看起来不自然。

6. 光晕（Halo Artifact）
定义：

光晕现象指的是在图像融合后的边界区域，特别是明暗交界的地方，出现了非自然的明亮或暗色环状伪影，看起来像一圈“光环”包裹着物体边缘。
在MEF中的表现：

在多曝光融合中，块级（patch-based）方法常常以小区域为单位进行处理，可能导致不同块之间的融合策略或权重不一致。当明暗区域交界时，不同块的处理结果拼接在一起，容易在边界处产生明显的光晕。例如，天空与建筑物交界的地方，会出现一圈不真实的亮带或暗带，影响图像的自然感和视觉质量。
7. 
8. 
9. 
10. 
11. 
12. 
13. 














总结栏







1. Grad-CAM：
[图片]

MEF 变换的方法
1. 变换域融合方法（Transform-domain Based Methods）
基本思路：

变换域方法首先将原始图像通过某种变换（如小波变换、金字塔分解等）转换到系数域（如频域或多尺度域），在该域内对各幅图像的系数进行融合处理，最后再通过反变换还原到原始图像域。
核心流程：
1. 变换：将多幅曝光图像变换到系数域。
2. 融合：在系数域内根据一定策略对不同图像的系数进行融合。
3. 逆变换：将融合后的系数通过反变换得到最终融合图像。
优点：
- 可以在不同尺度和频率下处理图像信息，有助于保留细节和结构。
- 更容易实现边缘保护和全局信息的融合。

---
4. 金字塔分解（Pyramid Decomposition）技术
2.1. 基本原理
金字塔分解是一种多尺度图像表示方法，将图像分解为不同分辨率的子带（subbands），常见的有高斯金字塔（Gaussian Pyramid）和拉普拉斯金字塔（Laplacian Pyramid）。
- 高斯金字塔：反复对图像进行低通滤波和下采样，得到一系列分辨率递减的图像。
- 拉普拉斯金字塔：在高斯金字塔基础上，计算每一层与上层放大后的差异，得到反映细节信息的拉普拉斯层。
2.2. Burt 等人的 MEF 金字塔融合方法
Burt 等人在 1983 年提出了拉普拉斯金字塔分解，并首次应用于图像融合（包括多曝光融合）。其基本流程如下：
1. 分解：对每幅输入图像进行拉普拉斯金字塔分解，得到多层次的图像细节表示。
2. 系数融合：对于每一层，采用某种融合规则（如取最大、加权平均等）对不同图像的同层系数进行融合。
3. 重建：将融合后的金字塔系数逐层上采样和叠加，最终重建出融合图像。
公式描述：
假设有 �N 幅输入图像 �1,�2,...,��I1,I2,...,IN，对每幅图像进行 �L 层拉普拉斯金字塔分解，得到系数 ��,�Li,k，其中 �i 表示第 �i 幅图像，�k 表示第 �k 层。融合系数 ��,�LF,k 可通过下式实现加权融合：
��,�(�,�)=∑�=1���,�(�,�)⋅��,�(�,�)LF,k(x,y)=i=1∑Nwi,k(x,y)⋅Li,k(x,y)
其中 ��,�(�,�)wi,k(x,y) 表示第 �i 幅图像在第 �k 层、(�,�)(x,y) 位置的权重。

---
4. 相关技术发展
金字塔分解是变换域融合的经典代表，后续还有如下变换域方法被广泛应用于MEF：
- 小波变换（Wavelet Transform）：多尺度、多方向分解，支持更丰富的频率分析。
- Curvelet/Contourlet 变换：更适合捕捉图像中的曲线和边缘结构。
- 稀疏表示与字典学习：在变换域通过稀疏编码融合图像特征。

---
5. 优缺点分析
- 优点：
  - 多尺度分析，细节与结构信息丰富。
  - 可以实现更自然的融合，减少伪影产生。
- 缺点：
  - 计算复杂度相对较高。
  - 融合规则设计不当时，仍可能出现光晕或过渡不自然等问题。

---
总结：

Burt 等人提出的金字塔分解方法是变换域融合在 MEF 领域的开创性工作，通过多尺度分解和系数域融合，有效提升了融合图像的细节表现和自然度。随着技术发展，变换域方法不断丰富和优化，成为多曝光融合的重要技术路线。
MEF-ssim
MEF-SSIM（Multi-Exposure Fusion Structural Similarity）是一种专门为多曝光图像融合（MEF）设计的客观质量评价指标，主要用于评价融合后图像与原始多曝光序列之间的结构、亮度和对比度的相似性。它是在传统**SSIM（结构相似性指标）**的基础上，根据MEF的特点进行了改进。
1. 传统SSIM公式回顾
传统SSIM定义如下：
SSIM(�,�)=[�(�,�)]�⋅[�(�,�)]�⋅[�(�,�)]�SSIM(x,y)=[l(x,y)]α⋅[c(x,y)]β⋅[s(x,y)]γ
其中：
- �(�,�)l(x,y) 亮度相似性
- �(�,�)c(x,y) 对比度相似性
- �(�,�)s(x,y) 结构相似性
具体表达式：
- �(�,�)=2����+�1��2+��2+�1l(x,y)=μx2+μy2+C12μxμy+C1
- �(�,�)=2����+�2��2+��2+�2c(x,y)=σx2+σy2+C22σxσy+C2
- �(�,�)=���+�3����+�3s(x,y)=σxσy+C3σxy+C3
其中 �μ、�σ 和 ���σxy 分别为均值、标准差和协方差，�1,�2,�3C1,C2,C3 为稳定常数。

---
2. MEF-SSIM的公式结构
MEF-SSIM 针对多曝光图像融合的特殊性进行了如下设计：
（1）信号分解
对于输入的每个图像补丁 ��xk，分解为：
- 信号强度 ��ck
- 信号结构 ��sk
- 平均强度 ��lk
（2）融合参考图像
MEF-SSIM不直接将融合结果与某一张原始图像比较，而是将融合图像与输入序列的“融合参考”进行比较。这个参考是在每个像素点上，选取该位置所有输入图像中最优的结构和对比信息进行组合，形成理想参考。
（3）MEF-SSIM局部指标
对于每个图像块，MEF-SSIM的结构如下：
MEF-SSIM=1�∑�=1�2��(�)��(�)+�1��(�)2+��(�)2+�1⋅2��(�)��(�)+�2��(�)2+��(�)2+�2⋅���(�)+�3��(�)��(�)+�3MEF-SSIM=N1i=1∑Nμf(i)2+μr(i)2+C12μf(i)μr(i)+C1⋅σf(i)2+σr(i)2+C22σf(i)σr(i)+C2⋅σf(i)σr(i)+C3σfr(i)+C3
其中：
- �f 为融合后的图像，�r 为理想参考图像
- �N 为所有图像块的数量
- ��(�),��(�)μf(i),μr(i) 分别为第 �i 个块的均值
- ��(�),��(�)σf(i),σr(i) 分别为第 �i 个块的标准差
- ���(�)σfr(i) 为第 �i 个块的协方差
- �1,�2,�3C1,C2,C3 为常数

---
3. MEF-SSIM的核心思想
- 不需要真实参考图像，而是利用输入序列自适应生成参考。
- 在局部块级别，综合考虑输入序列的结构和对比信息，形成理想参考，然后与融合结果进行对比。
- 评价结果越接近1，说明融合图像与理想参考越接近，融合质量越好。

---
4. 公式简化版
如果以单通道灰度图像为例，MEF-SSIM的简化公式可以写为：
MEF-SSIM=1�∑�=1�SSIM(��,��)MEF-SSIM=M1j=1∑MSSIM(fj,rj)
其中 ��fj 和 ��rj 分别为融合图像和理想参考在第 �j 个局部块的像素，�M 是块数。

---
5. 参考文献
- Ma, Kede, et al. "Multi-exposure image fusion by optimizing a structural similarity index." IEEE Transactions on Image Processing 24.12 (2015): 5370-5383.

NERF
NeRF（Neural Radiance Fields，神经辐射场）是一种用于三维场景表示和新视角合成的深度学习方法。其核心理论是通过一个神经网络来表示场景中的体积密度和颜色信息，从而实现从任意视角渲染逼真的图像。
关于NeRF的相关论文和理论，主要包括：
1. NeRF的基础论文：
  - Mildenhall等人在2020年发表的原始论文《NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis》，提出了用多层感知机（MLP）来拟合场景的体积密度和颜色，实现高质量的新视角合成。
2. 理论核心：
  - 通过输入空间坐标和视角方向，神经网络输出该点的颜色和体积密度。
  - 利用体积渲染技术，将沿射线的颜色和密度积分，合成最终像素颜色。
  - 训练时通过多视角图像的监督，优化网络参数。
3. 发展与应用：
  - 近年来，NeRF相关论文数量激增，涵盖加速训练与渲染、稀疏视角下的表现、动态场景建模、姿态估计、组合场景建模等多个方向。
  - 应用领域包括虚拟现实、增强现实、三维重建、人体动作捕捉等。
4. 资源推荐：
  - 有整理了大量NeRF相关论文的合集，如“2021-2023神经辐射场（NeRF）118篇顶会论文合集”，以及“101篇NeRF优质论文（含项目代码）”，这些资源对深入学习NeRF理论和最新进展非常有帮助。


SmoothGrad（平滑梯度）是一种用于提升深度学习模型解释性的方法，特别是在视觉模型中用于生成更清晰、更稳定的特征重要性可视化图（如Saliency Map）。它由2017年发表的论文《SmoothGrad: removing noise by adding noise》提出，旨在解决传统梯度敏感性图（saliency maps）噪声较大、难以解释的问题。
背景
深度学习模型通常被视为“黑盒”，输入图像经过复杂的网络后输出分类结果，但中间的决策依据不透明。为了理解模型的决策过程，研究者们提出了敏感性图（saliency maps）等方法，通过计算输入对输出的梯度，来反映输入中哪些像素对最终分类影响最大。
然而，传统的梯度敏感性图往往非常嘈杂，难以直观理解。SmoothGrad通过引入噪声平滑梯度，显著提升了可视化的质量。


SmoothGrad的核心思想
- 添加噪声：对输入图像多次添加不同的高斯噪声，生成多个带噪声的图像样本。
- 计算梯度：对每个带噪声的图像样本，计算模型输出相对于输入的梯度（即敏感性图）。
- 梯度平均：将所有带噪声样本的梯度图进行平均，得到一个平滑的梯度图。
通过这种方式，SmoothGrad能够减少梯度图中的随机噪声，使得最终的敏感性图更加清晰、稳定，突出模型真正关注的重要区域。
具体步骤
[图片]
优点
- 减少噪声：通过多次采样和平均，显著降低梯度图中的随机噪声。
- 提高可解释性：生成的敏感性图更平滑、更易于理解。
- 简单有效：方法简单，易于实现，且能与多种梯度解释方法结合使用。
应用场景
- 图像分类模型的解释
- 医疗影像分析中的特征重要性识别
- 任何需要理解深度模型决策依据的领域



SmoothGrad通过对输入添加噪声并计算梯度的平均值，来平滑梯度图，减少噪声，提高可解释性。而VarGrad则关注在多次添加噪声后梯度的方差，即梯度在不同噪声扰动下的变化程度。方差反映了模型对输入不同部分的敏感性稳定性，揭示了梯度不确定性和模型对输入扰动的响应差异。
VarGrad的定义
给定输入 �x，从均值为0、方差为 �2σ2 的正态分布中独立同分布采样噪声向量 ��∼�(0,�2)gi∼N(0,σ2)，对每个带噪声的输入 �+��x+gi 计算梯度：
��=∇��(�+��)gi=∇xf(x+gi)
VarGrad定义为这些梯度的方差：
VarGrad(�)=Var(��)=�[(��−�[��])2]VarGrad(x)=Var(gi)=E[(gi−E[gi])2]
其中，�E 表示期望，VarVar 表示方差。
[图片]
直观理解
- SmoothGrad 关注的是梯度的平均值，强调模型对输入的整体敏感性。
- VarGrad 关注的是梯度的方差，强调模型对输入扰动的敏感性变化，即哪些区域的梯度在不同噪声扰动下变化较大，反映了模型对这些区域的不确定性或不稳定性。

SHAP
1. 概念简介
SHAP值是一种用于解释机器学习模型输出的方法。它基于合作博弈论中的Shapley值理论，能够为每个特征分配一个对模型预测结果的“贡献度”，从而帮助我们理解模型的决策依据。

---
2. 原理基础
- Shapley值原本用于合作博弈论，用于衡量每个玩家在团队合作中对总收益的贡献。
- 在机器学习场景下，将每个特征视为“玩家”，模型输出视为“收益”。SHAP值衡量每个特征对预测结果的边际贡献。
计算公式
SHAP值的数学表达为：
[图片]
��=∑�⊆�∖{�}∣�∣!(∣�∣−∣�∣−1)!∣�∣![��∪{�}(��∪{�})−��(��)]ϕi=S⊆F∖{i}∑∣F∣!∣S∣!(∣F∣−∣S∣−1)![fS∪{i}(xS∪{i})−fS(xS)]
- �F：所有特征的集合
- �S：特征子集，不包含特征�i
- ��(��)fS(xS)：只用�S特征时模型的预测
- ��∪{�}(��∪{�})fS∪{i}(xS∪{i})：加上�i特征后的预测结果

---
3. SHAP的优点
- 全局和局部解释：既可以解释单个预测（局部），也可以解释整体模型（全局）。
- 一致性：如果某个特征对预测的贡献提高，其SHAP值不会降低。
- 模型无关性：适用于任意机器学习模型（树模型、神经网络等）。

盲去噪与非盲去噪的异同

暂时无法在HONOR E Link文档外展示此内容


---
简要总结
- 盲去噪：去噪时不需要知道噪声的类型和强度，更贴近实际复杂环境，对算法的自适应能力要求高。
- 非盲去噪：去噪时需要已知噪声的类型和参数，可以针对性优化，去噪效果通常更好，但适用范围有限。

---
一句话总结：
盲去噪无需噪声先验，适用性强但难度大；非盲去噪依赖噪声信息，效果好但适用性有限。

卷积层反向传播
卷积神经网络（CNN）中的卷积层反向传播主要是计算损失函数对卷积层参数（卷积核权重）和输入的梯度，以便进行参数更新。具体步骤如下：
1. 前向传播回顾

卷积层的输出是输入与卷积核的卷积结果，通常表示为：
2. �=�∗�+�y=x∗w+b
3. 其中，�x 是输入，�w 是卷积核权重，�b 是偏置，∗∗ 表示卷积操作。
4. 反向传播目标

给定损失函数对输出的梯度 ∂�∂�∂y∂L，计算：
  - 输入的梯度 ∂�∂�∂x∂L
  - 权重的梯度 ∂�∂�∂w∂L
  - 偏置的梯度 ∂�∂�∂b∂L
5. 计算权重梯度

权重梯度是输入与输出梯度的卷积：
6. ∂�∂�=�∗∂�∂�∂w∂L=x∗∂y∂L
7. 这里的卷积操作通常是输入与输出梯度的“互相关”操作。
8. 计算输入梯度

输入梯度是输出梯度与卷积核的旋转180度后的卷积：
9. ∂�∂�=∂�∂�∗����∂x∂L=∂y∂L∗wrot
10. 其中 ����wrot 是卷积核旋转180度。
11. 计算偏置梯度

偏置梯度是输出梯度在空间维度上的求和：
12. ∂�∂�=∑∂�∂�∂b∂L=∑∂y∂L
[图片]
sd模型
一般来说，N 函数表示采样过程，前向过程与反向过程，都可以用此表示，不过两者有点细微的区别。反向过程（即去噪的过程），N 需要包含均值与方差的计算。而前向过程不需要。
[图片]
扰动像素的显著性评价方法

暂时无法在HONOR E Link文档外展示此内容
1. 方法的变体
- 移除最显著像素（deletion/insertion）：可以逐步移除或插入像素，绘制输出变化曲线。
- 像素替换方式：可以用黑色、灰色、均值、随机噪声等替换像素。

---
2. 存在的问题和局限性
- 高频边缘伪影（edge artifacts）：
  - 直接把像素替换成黑色或其他固定值，可能会在图像中产生不自然的边界或“洞”，这些高频伪影本身就可能影响模型的输出。
  - 输出变化有可能并不是因为移除了“重要像素”，而是因为引入了模型不适应的异常输入。
- 显著性算法输出差异不大：
  - 实验中发现，不同显著性方法在这种评测下，输出变化可能很接近，难以区分算法优劣。


ROAR
1. 方法简介
ROAR（RemOve And Retrain）是一种评估显著性算法（saliency method）解释模型行为能力的近似基准方法。它的核心思想是：如果显著性方法能正确找到对模型最重要的像素，那么移除这些像素后重新训练的模型性能会显著下降。

---
2. 具体步骤
步骤流程
1. 对整个数据集，用显著性方法对每张图片生成显著性图。
2. 选取前 zk% 最显著的像素，把它们从所有图片中“移除”或“替换”为某个无信息值（如均值、随机噪声等）。
3. 用处理后的新数据集，重新训练一个和原来结构相同的分类器。
4. 比较新模型和原始模型的准确率（accuracy）。

---
5. 评价标准与公式
1. 准确率下降量（Accuracy Drop）
设：
- �D ：原始数据集
- �′D′ ：移除 �%k% 重要像素后的数据集
- �f ：原始训练好的模型
- �′f′ ：在 �′D′ 上重新训练的模型
- �����Aorig ：�f 在原始测试集上的准确率
- �����AROAR ：�′f′ 在新测试集（同样经过像素移除处理）上的准确率
则，准确率下降量为：
Δ�=�����−�����ΔA=Aorig−AROAR
- Δ�ΔA 越大，说明该显著性方法越能准确找出重要像素。

---
2. 数据处理公式
假设对于一张图片 �x，显著性图为 �(�)S(x)，�(�)�S(x)i 表示第 �i 个像素的重要性分数。
- 选出 �%k% 最大的 �(�)�S(x)i，对这些像素进行替换（如设为均值�μ、零值等），得到新图片 �′x′。
用集合 �M 表示被移除像素的索引：
��′={�,if �∈���,otherwisexi′={μ,xi,if i∈Motherwise

---
6. 方法优缺点
优点
- 删除伪影影响小：由于重新训练模型，减少了像素移除带来的高频伪影对模型输出的直接影响。
- 更关注真正重要像素的识别能力。
缺点
- 引入新线索：模型可以学习到“哪些像素被移除了”这个新特征，从而间接影响分类准确率。
- 计算代价高：每次评测都需要重新训练模型，成本较高。

---
7. 与像素扰动法对比
- 像素扰动法（Pixel Perturbation）：只测试删除不重要区域时模型输出的稳定性。
- ROAR：强调识别重要像素，删除后重新训练，关注准确率下降。

---
8. 公式与流程小结
[图片]


---
关键词强调
- 移除 k% 最显著像素
- 重新训练模型
- 准确率下降量 
- 关注于识别重要像素的能力

常见IQA指标
1. LPIPS（Learned Perceptual Image Patch Similarity）
- 简介：LPIPS 是一种基于深度学习特征的图像感知相似度指标，常用于评价生成图像与参考图像的感知差异。
- 原理：将两张图像输入同一个预训练的卷积神经网络（如 VGG、AlexNet），在多个层提取特征，然后计算特征差异的加权距离。
- 公式：
- LPIPS(�,�)=∑���⋅1����∑ℎ,�∥��(�)ℎ�−��(�)ℎ�∥22LPIPS(x,y)=l∑wl⋅HlWl1h,w∑∥Fl(x)hw−Fl(y)hw∥22
- 其中 ��(⋅)Fl(⋅) 表示网络第 �l 层的特征，��wl 是该层的权重，��,��Hl,Wl 是特征图的高和宽。

---
2. FID（Fréchet Inception Distance）
- 简介：FID 主要用于评价生成模型（如 GAN）生成的图像与真实图像的整体分布相似度。
- 原理：提取生成图像和真实图像的 Inception 模型特征，假设特征服从高斯分布，计算两组分布的 Fréchet 距离。
- 公式：
- FID=∥��−��∥2+Tr(Σ�+Σ�−2(Σ�Σ�)1/2)FID=∥μr−μg∥2+Tr(Σr+Σg−2(ΣrΣg)1/2)
- 其中 ��,Σ�μr,Σr 是真实图像特征的均值和协方差，��,Σ�μg,Σg 是生成图像特征的均值和协方差。

---
3. NIQE（Natural Image Quality Evaluator）
- 简介：NIQE 是一种无参考图像质量评价指标，利用自然场景统计特征来衡量图像质量。
- 原理：统计图像局部块的 NSS 特征，与自然图像统计模型的分布做对比，计算马氏距离。
- 公式：
- NIQE(�)=(�−��)�Σ�−1(�−��)NIQE(I)=(x−μn)TΣn−1(x−μn)
- 其中 �x 为测试图像的统计特征，��,Σ�μn,Σn 为自然图像的均值和协方差。

---
4. NRQM (Ma)（No-Reference Quality Metric by Ma et al.）
- 简介：Ma 指标是一种无参考的感知质量评价方法，常用于超分辨率等领域。
- 原理：通过特征提取和训练得到的回归模型，预测图像的主观质量分数，分值范围一般为[0, 10]。
- 公式：Ma 指标本身没有公开的简单公式，常用深度网络回归得到分数，记为
- Ma(�)=��(�)Ma(I)=fθ(I)
- 其中 ��fθ 是训练好的回归模型。

---
5. MUSIQ（Multi-scale Image Quality）
- 简介：MUSIQ 是一种多尺度无参考图像质量评价指标，适用于各种分辨率的图像。
- 原理：利用多尺度特征提取+Transformer结构，最终输出质量分数。
- 公式：MUSIQ 依赖深度网络，分数为
- MUSIQ(�)=������(�)MUSIQ(I)=fMUSIQ(I)
- 其中 ������fMUSIQ 是训练好的多尺度网络。

---
6. NIMA（Neural Image Assessment）
- 简介：NIMA 是基于深度学习的无参考图像美学/质量评价模型。
- 原理：用 CNN 预测图像美学或质量打分分布，最终输出均值或加权分值。
- 公式：
- NIMA(�)=∑�=110��⋅��NIMA(I)=i=1∑10pi⋅si
- 其中 ��pi 是预测的概率分布，��si 是1-10的分数。

---
7. DBCNN（Deep Bilinear Convolutional Neural Network）
- 简介：DBCNN 是基于深度双线性卷积神经网络的无参考图像质量评价方法。
- 原理：通过双线性池化提取更丰富的特征后，回归预测图像质量分数。
- 公式：
- DBCNN(�)=������(�)DBCNN(I)=fDBCNN(I)
- 其中 ������fDBCNN 是训练好的深度网络。

---
8. WaDIQaM（Weighted Average Deep Image Quality Measure）
- 简介：WaDIQaM 采用分块、加权平均的深度学习无参考图像质量评价方法。
- 原理：图像被分为多个块，每块通过深度网络预测分数，再加权平均得到整体分数。
- 公式：
- WaDIQaM(�)=∑�=1���⋅��(��)WaDIQaM(I)=k=1∑Kwk⋅fk(Ik)
- 其中 ��Ik 是第 �k 个图像块，��fk 是该块的预测分数，��wk 是权重。

---
9. BRISQUE（Blind/Referenceless Image Spatial Quality Evaluator）
- 简介：BRISQUE 是一种基于空间域自然场景统计的无参考图像质量评价方法。
- 原理：提取空间域 NSS 特征，利用 SVR（支持向量回归）预测图像质量分数。
- 公式：
- BRISQUE(�)=SVR(NSS Features(�))BRISQUE(I)=SVR(NSS Features(I))
- 其中 SVR 是训练好的回归模型。

---
10. PI（Perceptual Index）
- 简介：PI 综合了 Ma 指标和 NIQE 指标，常用于超分辨率等主观与客观兼顾的场景。
- 原理：通过简单线性组合，两者得分越低，感知质量越高。
- 公式：
- PI(�)=12[(10−Ma(�))+NIQE(�)]PI(I)=21[(10−Ma(I))+NIQE(I)]



KAN（K-Activated Neural Networks）的详细解释
1. 基本概念：

KAN是一种用**可学习的分段多项式（K-activation）**替换传统激活函数（如ReLU、Sigmoid、Tanh）的方法。

在KAN中，每个神经元的激活函数不是固定的，而是通过学习得到的分段多项式，可以更灵活地拟合复杂的非线性关系。
2. 结构原理：
- 传统的神经网络每一层都是线性变换+非线性激活。
- KAN 用分段多项式激活函数替换了传统的激活函数，这些激活函数的参数在训练过程中自动优化。
- 这种方式增强了网络的表达能力和适应性。
3. 数学表达：

[图片]
假设输入为 �x，传统激活函数是 �(�)f(x)，而 KAN 中，激活函数变成：
�(�)=∑�=1�����(�)f(x)=k=1∑KakBk(x)
其中 ��(�)Bk(x) 表示第 �k 个基函数（如分段多项式），��ak 为可训练参数，�K 为分段数。
4. 优势：
- 表达能力更强，可以自适应学习最优的激活函数。
- 更高的拟合和泛化能力，尤其在复杂任务中表现突出。
- 对传统网络的兼容性好，可以直接替换激活函数模块。
5. 代表论文：
- 2024年，论文 "K-Activated Neural Networks"（KANs）首次系统性提出了这一结构，并在多个任务上取得了突破性结果。

MS-SSIM
1. MS-SSIM 是什么？
MS-SSIM 是结构相似性指标（SSIM, Structural Similarity Index）的扩展版本。

SSIM 用于衡量两幅图像在结构、亮度和对比度方面的相似性。而 MS-SSIM（Multi-Scale SSIM）进一步引入了多尺度的思想，通过对图像进行多次下采样，融合不同分辨率下的相似性信息，使得评估更加全面和鲁棒。

---
2. MS-SSIM 的原理
- 单尺度 SSIM 只在单一分辨率（原图）上计算结构、亮度和对比度的相似性。
- 多尺度 MS-SSIM 则将图像逐步下采样（通常每次下采样为原来的一半），在每个尺度上分别计算 SSIM，最后对各尺度结果加权融合。
这样做的好处：
- 能更好地反映出图像在不同分辨率下的结构信息差异。
- 更符合人眼视觉特性（人眼对不同尺度的信息都敏感）。

---
3. 计算流程
4. 对原始图像进行多次下采样，得到不同尺度的图像（比如 5 个尺度）。
5. 在每个尺度上分别计算 SSIM，包括亮度、对比度和结构三个分量。
6. 对每个尺度的结果按一定权重（通常高分辨率权重小，低分辨率权重大）进行加权乘积融合，得到最终的 MS-SSIM 分数。

---
7. 公式
假设有 �M 个尺度，第 �j 个尺度的亮度、对比度、结构分量分别为 ��,��,��lj,cj,sj，则 MS-SSIM 的整体公式为：
[图片]
SIC 与AIC
SIC：计算不同阈值下所筛选出来的集合 的平均 信息量（香浓公式，根据概率计算出）
AIC： 计算不同阈值下，sic与 准确率的关系
Softmax Information Curves (SIC) 和 Accuracy Information Curves (AIC) 是结合信息论和分类准确率的分析工具，用于评估分类模型在不同置信度阈值下的表现。下面给出更详细的定义、计算过程和公式。

---
1. Softmax Information Curves (SIC)
1.1 背景
模型对输入样本的预测通常是一个类别概率分布，通常通过 softmax 函数得到：
��=softmax(��)=���∑�=1����pi=softmax(zi)=∑j=1Cezjezi
其中，��zi 是模型对类别 �i 的原始输出（logits），�C 是类别数。
1.2 信息量计算
信息论中，某事件发生的“信息量”定义为该事件的负对数概率：
��=−log⁡��Ii=−logpi
这里，��pi 是模型对预测类别的置信度（概率）。信息量越大，表示该预测越“不确定”或“罕见”。
1.3 SIC 曲线的构造
- 对测试集中的每个样本，计算其预测类别的置信度 ��pi 和对应信息量 ��Ii。
- 设定一系列置信度阈值 �τ，例如从 0 到 1。
- 对于每个阈值 �τ，筛选出所有置信度 ��≥�pi≥τ 的样本集合 ��Sτ。
- 计算该集合的平均信息量：
SIC(�)=1∣��∣∑�∈����=−1∣��∣∑�∈��log⁡��SIC(τ)=∣Sτ∣1i∈Sτ∑Ii=−∣Sτ∣1i∈Sτ∑logpi
- 绘制 �τ 与 SIC(�)SIC(τ) 的曲线，即为 Softmax Information Curve。
1.4 解释
- 随着阈值 �τ 增大，筛选出的样本置信度更高，平均信息量通常下降（预测更确定）。
- SIC 曲线反映模型在不同置信度水平下预测结果的信息含量。

---
2. Accuracy Information Curves (AIC)
2.1 背景
AIC 结合置信度阈值和准确率，分析模型在不同置信度水平下的准确率表现。
2.2 准确率计算
- 对于阈值 �τ，筛选置信度 ��≥�pi≥τ 的样本集合 ��Sτ。
- 计算该集合的准确率：
AIC(�)=1∣��∣∑�∈��1(�^�=��)AIC(τ)=∣Sτ∣1i∈Sτ∑1(y^i=yi)
其中，�^�y^i 是模型预测类别，��yi 是真实类别，1(⋅)1(⋅) 是指示函数，预测正确为1，否则为0。
2.3 AIC 曲线的构造
- 设定一系列阈值 �τ。
- 计算每个阈值对应的准确率 AIC(�)AIC(τ)。
- 绘制 �τ 与 AIC(�)AIC(τ) 的曲线。
2.4 解释
- 随着阈值增大，筛选出的样本置信度更高，准确率通常提升。
- AIC 曲线帮助选择合适的置信度阈值，平衡准确率和覆盖率。

---
3. 计算流程总结
假设测试集有 �N 个样本，模型输出 softmax 概率 ��pi，预测类别 �^�y^i，真实类别 ��yi。
1. 计算信息量：
2. ��=−log⁡��Ii=−logpi
3. 设定阈值序列：
4. �∈{�1,�2,...,��},0≤�1<�2<...<��≤1τ∈{τ1,τ2,...,τM},0≤τ1<τ2<...<τM≤1
5. 对每个阈值 �τ：
  - 筛选样本集合：
  - ��={�∣��≥�}Sτ={i∣pi≥τ}
  - 计算平均信息量（SIC）：
  - SIC(�)=1∣��∣∑�∈����SIC(τ)=∣Sτ∣1i∈Sτ∑Ii
  - 计算准确率（AIC）：
  - AIC(�)=1∣��∣∑�∈��1(�^�=��)AIC(τ)=∣Sτ∣1i∈Sτ∑1(y^i=yi)
6. 绘制曲线：
  - 横轴为阈值 �τ。
  - 纵轴分别为 SIC(�)SIC(τ) 和 AIC(�)AIC(τ)。

常见的归因分析方法
一、按归因原理分类
1. 基于梯度的方法（Gradient-based Methods）
- Saliency Map
直接计算模型输出对输入的梯度，�=∂�(�)∂�S=∂x∂f(x)，高梯度位置即为重要区域。
- Gradient × Input
将输入乘以其梯度，�=�⋅∂�(�)∂�S=x⋅∂x∂f(x)。
- Integrated Gradients
沿输入到基线之间的路径积分梯度：
- ���(�)=(��−��′)∫�=01∂�(�′+�(�−�′))∂����IGi(x)=(xi−xi′)∫α=01∂xi∂f(x′+α(x−x′))dα
- 其中�′x′为基线。
- Guided Backpropagation
修改反向传播过程，仅传递正向梯度。
- SmoothGrad
对输入加噪声多次，归因结果取平均，提升鲁棒性。
2. 基于前向传播的方法（Propagation-based Methods）
- DeepLIFT (Deep Learning Important FeaTures)
比较输入与参考输入的激活差异，分摊输出变化。（即前向传播中，计算x 与x‘ 不同 神经元计算的结果差异，再分摊输出变化，即将结果映射）
- Layer-wise Relevance Propagation (LRP)
沿网络层级将输出相关性分摊回输入。
3. 基于扰动的方法（Perturbation-based Methods）
- Occlusion / Masking
局部遮挡输入，观察模型输出变化。
- Shapley Value-based Methods (如SHAP)
组合输入子集，计算各特征边际贡献。
4. 基于代理模型的方法（Surrogate/Proxy Methods）
- LIME (Local Interpretable Model-agnostic Explanations)
在局部输入邻域训练可解释模型（如线性模型）拟合原模型。
MEF 的痛点
1. LDR图像的严重错位（Severe Misalignment）
原理
- 多曝光融合/多帧HDR需要将多张低动态范围（LDR）图像对齐（register），以便融合每张图像的优势区域。
- 当前景物体有大幅度运动时，不同曝光的LDR图像之间会出现明显的错位（misalignment）。
- 错位会导致融合时出现重影（ghosting）、模糊等伪影。
技术
- 运动估计与补偿（Motion Estimation & Compensation）：
  - 光流（Optical Flow）：计算各帧之间的像素级运动矢量，用于对齐。
  - 特征点匹配（Feature Matching）：检测、匹配关键点，进行全局或局部对齐。
  - 局部块匹配（Patch-based Matching）：以小块为单位进行匹配和对齐，适合处理局部运动。
- 对齐算法（Alignment Algorithms）：
  - 基于全局变换（如仿射、透视变换）的方法适合相机轻微抖动。
  - 基于局部变换或分块的方法适合前景运动。
- 无重影融合（De-ghosting Fusion）：
  - 利用运动掩码区分静态/动态区域，对动态区域采用特殊融合策略。

---
2. 运动物体导致的曝光区域内容缺失（Missing Content）
原理
- 运动物体在不同曝光下可能出现在不同的位置，且某些区域会因过曝（over-saturation）或欠曝（under-saturation）而丢失细节。
- 这些丢失区域在所有曝光帧中都无法完全补偿，造成融合时内容缺失（missing content）。
技术
- 内容恢复与补全（Content Restoration/Inpainting）：
  - 基于深度学习的内容生成（如GAN、Diffusion模型）自动补全缺失区域。
  - 多帧信息融合，利用其它帧中未丢失的信息进行补偿。
- 动态区域检测与自适应融合：
  - 检测运动区域，采用不同的融合权重或算法。
  - 静态区域采用常规融合，动态区域可用运动补偿或内容补全方法处理。
- 曝光一致性增强（Exposure Consistency Enhancement）：
  - 对欠曝/过曝区域做局部增强，提升细节恢复能力。


HDR domain
HDR domain（高动态范围域）是指在图像处理、视频、显示等领域中，以高动态范围（High Dynamic Range, HDR）为基础进行数据表示、处理或分析的空间或范畴。HDR domain 的核心在于对图像或视频内容中亮度和色彩的极大范围进行真实还原和表达，以接近人眼对自然场景的感知效果。
一、HDR domain的基本概念
1. 动态范围（Dynamic Range）

动态范围是指系统能够记录或显示的最大亮度与最小亮度的比值。传统的SDR（Standard Dynamic Range）设备动态范围有限，通常丢失高光和阴影细节；而HDR技术可以捕捉和显示更宽广的亮度范围。
2. HDR domain的意义
  - 在HDR domain内，每个像素的亮度和色彩值可以超出传统8bit或10bit的范围，甚至可以表示物理世界中真实的光照强度。
  - HDR domain常用于图像合成、色调映射（Tone Mapping）、渲染、显示等环节。
二、HDR domain的数学表达
3. 动态范围的计算公式
动态范围通常用对数表示：
[图片]
Dynamic Range (DR)=log⁡10(��������)Dynamic Range (DR)=log10(LminLmax)
- ����Lmax：系统可表达的最大亮度
- ����Lmin：系统可表达的最小亮度
例如，自然场景的动态范围可以高达 105:1105:1，而传统显示设备通常只有 102:1102:1。
4. HDR合成的核心公式
HDR合成的目标是恢复原始场景的辐照度图（irradiance map）。常见的像素级合成公式如下：
[图片]
�(�)=∑�=1��(��(�))⋅��(�)−��∑�=1��(��(�))E(x)=∑i=1Nw(Zi(x))∑i=1Nw(Zi(x))⋅aZi(x)−b
- �(�)E(x)：像素�x的实际辐照度
- ��(�)Zi(x)：第�i张不同曝光图片在像素�x的灰度值
- �,�a,b：相机响应曲线参数
- �(⋅)w(⋅)：权重函数（通常对中间灰度值赋予较高权重）
5. HDR domain常用传递函数
- OETF（光电转换函数）：将场景亮度映射为数字信号
- EOTF（电光转换函数）：将数字信号还原为显示亮度
常见的HDR传递函数有：
- PQ（Perceptual Quantizer），标准为ST-2084
- HLG（Hybrid Log-Gamma）
PQ曲线的核心公式为：
�=����⋅(max⁡[(�1/�2−�1�2−�3�1/�2),0]1)1/�1L=Lmax⋅1max[(c2−c3V1/m2V1/m2−c1),0]1/m1
- �L：输出亮度
- �V：输入信号
- ����Lmax、�1m1、�2m2、�1c1、�2c2、�3c3：标准定义的常数
三、HDR domain的应用
- 摄影与后期：多张不同曝光的图片合成到HDR domain，最大化细节保留
- 视频编码与播放：以HDR domain为基准进行内容制作和显示，支持更丰富的明暗和色彩层次
- 显示设备：HDR显示器、电视等直接支持HDR domain的数据输入与输出
四、HDR domain与SDR domain的区别
- SDR domain：通常8bit/10bit，亮度范围有限，细节容易丢失
- HDR domain：高比特深度（10bit/12bit/16bit及以上），覆盖更广的亮度和色彩空间，细节丰富

---
总结：
HDR domain是指以高动态范围为基础进行图像或视频处理的空间，能够表达和处理远超传统SDR的亮度与色彩范围。其核心在于利用对数动态范围、物理级光照强度、HDR传递函数等数学模型，实现更接近真实世界的视觉体验。

HDR 传递函数
HDR传递函数（HDR Transfer Function）是指在高动态范围（HDR, High Dynamic Range）视频和图像处理流程中，用于描述和控制图像信号的亮度与数字编码值之间映射关系的数学函数。它决定了如何将真实场景中的物理亮度（光）转换为数字信号（编码），以及如何将数字信号还原为显示设备上的亮度。

---
常见HDR传递函数类型
1. OETF（Opto-Electronic Transfer Function，光电转换函数）
  - 作用： 场景光强 → 数字信号
  - 用途： 摄像机/图像采集端将真实光线转为数字编码
2. EOTF（Electro-Optical Transfer Function，电光转换函数）
  - 作用： 数字信号 → 显示亮度
  - 用途： 显示设备将数字编码还原为可见光亮度
3. OOTF（Opto-Optical Transfer Function，光光转换函数）
  - 作用： 场景光 → 显示光的整体映射
  - 用途： 用于描述采集到显示的整体视觉映射

Reference-based residual structure 的结构
其实就是只加了部分的输入
详细解释
1. Residual Structure（残差结构）
  - 残差结构起源于ResNet，核心思想是让网络学习输入与输出之间的“差异”（即残差），而不是直接学习输出本身。
  - 其基本公式为：
  - Output=�(�)+�Output=F(x)+x
  - 其中，�(�)F(x) 表示网络学习到的残差信息，�x为输入。
2. Reference-based（基于参考）
  - 传统残差结构仅依赖于输入自身，而reference-based结构会引入一个“参考”图像或特征，即利用参考数据指导残差的学习。
  - 这种结构通常用于风格迁移、超分辨率、图像修复、视频帧补全等任务，比如将一张低分辨率图像与一张高分辨率参考图像对齐，指导网络学习更精细的细节。

---
结构原理举例
以Reference-based Image Super-Resolution（基于参考的图像超分辨率）为例：
- 输入： 低分辨率图像 ���ILR 和参考高分辨率图像 ����IRef
- 网络目标： 输出高分辨率图像 ���ISR，使其在结构和细节上都更接近真实高分辨率
- 核心思想： 通过对齐和融合 ���ILR 与 ����IRef 的特征，网络学习如何将参考图像中的信息“补充”到输入图像中
- 残差公式：
- ���=���↑+�(���↑,����)ISR=ILR↑+F(ILR↑,IRef)
  - ���↑ILR↑：上采样后的低分辨率图像
  - �F：网络学习到的“参考引导下”的残差信息


μ-law色调映射公式的具体作用
μ-law色调映射公式如下：
[图片]
�(�)=log⁡(1+��)log⁡(1+�)T(x)=log(1+μ)log(1+μx)
其中：
- �x：归一化后的输入像素值，�∈[0,1]x∈[0,1]
- �μ：压缩参数，�0μ0
作用总结：
1. 动态范围压缩：

μ-law色调映射将像素值的动态范围进行非线性压缩，把高动态范围（HDR）的亮度信息映射到 [0,1][0,1] 区间，方便普通显示设备显示。
2. 增强暗部细节：

由于对低亮度值的响应更敏感（曲线陡峭），μ-law能够增强暗部细节，使得图像的暗部信息在压缩后更加明显。
3. 更符合人眼感知：

人眼对亮度变化的感知是对数型的，μ-law映射模拟了这种感知，使得经过色调映射后的图像更加自然、真实。
4. 便于网络训练：

在深度学习中，μ-law是可微分的，便于损失函数的反向传播，有助于网络更好地优化。

EV（曝光值，Exposure Value）

EV 是衡量相机曝光量的一个参数。它综合了光圈（Aperture）、快门速度（Shutter Speed）和ISO感光度，用来表示一张照片的明暗程度。

在HDR拍摄中，通常会拍摄多张不同EV的图片（即不同曝光设置），以捕捉场景的明暗细节，最终合成一张细节丰富的HDR图像。

---
原理
1. EV的计算公式
      
   $$
EV = \log_2\left(\frac{N^2}{t}\right)
$$
其中，$N$ 表示光圈值（f-number），$t$ 表示快门时间（单位：秒）。
---
考虑 ISO 的曝光值：
$$
EV_{ISO} = \log2\left(\frac{N^2}{t}\right) - \log2\left(\frac{ISO}{100}\right)
$$  
                        
         


2. EV的意义
- EV数值越大，表示曝光量越小（照片越暗），通常由更小的光圈、更快的快门或更低的ISO实现。
- EV数值越小，表示曝光量越大（照片越亮），通常由更大的光圈、更慢的快门或更高的ISO实现。
3. HDR中的应用
- HDR拍摄：通常用不同EV设置连续拍摄多张照片（如 −2−2 EV、00 EV、+2+2 EV），以覆盖场景的全部亮度范围。
- 合成HDR：将多张不同EV的照片融合，保留每一张中最优的亮度细节，生成一张高动态范围的图像。

2、小问题：
使用 多线程时，multiprocessing库函数。 只能转递一个参数，所以线程函数需要多个参数，需要利用字典等传参，或者是args；
多线程返回时，只能返回一个result，多个参数需要用zip(*result)解析。
线程函数解析后的数值，是一个列表，包含 n个线程（总的任务数）的结果。
   with Pool(processes=num_gpus) as pool:
        results = pool.map(process_model_file, batch_args)  阻塞直到本批任务完成
  Map 即是阻塞
  如果有返回值，此方法可能会造成大量的内存占用。需要用imap_unordered 或者imap 来实现
  with Pool(processes=num_gpus) as pool:imap_unordered 适合任务耗时不均
    for result in pool.imap_unordered(process_model_file, args_list):
        grad, infer_result, gt, name = result
        grad_all_list.extend(list(itertools.chain.from_iterable(grad)))
        infer_result_all_list.extend(list(itertools.chain.from_iterable(infer_result)))
        name_list.extend(list(itertools.chain.from_iterable(name)))
